{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Introduction\n",
        "PANet is a semantic image segmentation model developed in 2019 that utilizes path augmented regularization (PAR) alsongside other novel approaches to out perform predacessor networks by a large margin.\n",
        "\n",
        "This notebook is a supplement aiming to give insight into the network training pipeline.\n",
        "In this notebook, we will initialize all files and values, and then we'll perform a single training run on a single split of data.\n",
        "We will then use that trained model to perform image segmentation on the test images to obtain qualitative results."
      ],
      "metadata": {
        "id": "af-sqsDAJqn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7Ei-DvM1Px9",
        "outputId": "f7cd8121-d242-419b-b485-2ba12effb47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\n",
            "Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 97 (delta 3), reused 3 (delta 3), pack-reused 90 (from 2)\u001b[K\n",
            "Unpacking objects: 100% (97/97), 39.59 KiB | 587.00 KiB/s, done.\n",
            "From https://github.com/bryjen/PANet\n",
            " * [new branch]      master     -> origin/master\n",
            "HEAD is now at bf60b2c Fixes training loss tracker\n",
            "Collecting sacred\n",
            "  Downloading sacred-0.8.7-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting docopt-ng<1.0,>=0.9 (from sacred)\n",
            "  Downloading docopt_ng-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jsonpickle>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from sacred) (4.0.5)\n",
            "Collecting munch<5.0,>=2.5 (from sacred)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from sacred) (1.17.2)\n",
            "Requirement already satisfied: py-cpuinfo>=4.0 in /usr/local/lib/python3.11/dist-packages (from sacred) (9.0.0)\n",
            "Collecting colorama>=0.4 (from sacred)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.11/dist-packages (from sacred) (24.2)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (from sacred) (3.1.44)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython->sacred) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython->sacred) (5.0.2)\n",
            "Downloading sacred-0.8.7-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading docopt_ng-0.9.0-py3-none-any.whl (16 kB)\n",
            "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Installing collected packages: munch, docopt-ng, colorama, sacred\n",
            "Successfully installed colorama-0.4.6 docopt-ng-0.9.0 munch-4.0.0 sacred-0.8.7\n"
          ]
        }
      ],
      "source": [
        "# @title Init Project Code, Imports, and Constants\n",
        "!pip install torch-summary\n",
        "!pip install torchinfo\n",
        "\n",
        "!git init\n",
        "!git remote add origin https://github.com/bryjen/PANet\n",
        "!git fetch\n",
        "!git reset --hard origin/master\n",
        "\n",
        "!pip install sacred\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import torch\n",
        "import shutil\n",
        "import zipfile\n",
        "import kagglehub\n",
        "import torch.optim\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from google.colab import drive\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "from models.fewshot import FewShotSeg\n",
        "from dataloaders.customized import voc_fewshot, coco_fewshot\n",
        "from dataloaders.transforms import ToTensorNormalize\n",
        "from dataloaders.transforms import Resize, DilateScribble\n",
        "from util.metric import Metric\n",
        "from util.utils import set_seed, CLASS_LABELS, get_bbox\n",
        "from config import ex\n",
        "\n",
        "DRIVE_MOUNTED = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pre-trained and Weights & Dataset augmentation files\n",
        "# Here we download files required to setup the pipeline.\n",
        "# We download pre-trained weights so that we can fine-tune the model.\n",
        "# Additionally, we download additional annotations (scribbles + bounding boxes)\n",
        "# and modify the downloaded dataset with these files.\n",
        "\n",
        "# pre-trained weights\n",
        "os.makedirs(\"/content/pretrained_model/\", exist_ok=True)\n",
        "!gdown 1TJZy_YkYwNMkdtECAlkWy8VuorPFjP1t --output \"/content/pretrained_model/\"\n",
        "\n",
        "# dataset modification files\n",
        "os.makedirs(\"/content/dataset_augmentations/\", exist_ok=True)\n",
        "!gdown 1ZP6FHiclSNk1nH0WxhCH-W_fwd3GnEZa --output \"/content/dataset_augmentations/\"\n",
        "!gdown 1oGy-tg_Tv_-ZUQeiNtM5i4sK0lYB0MeL --output \"/content/dataset_augmentations/\"\n",
        "!gdown 1R5G3BfQTAY4zWkazTsgFc3kJNQVjwi4M --output \"/content/dataset_augmentations/\"\n",
        "!gdown 1D4oX0Ub6ObFO81yonWY4ehZ-tr5Hn5UK --output \"/content/dataset_augmentations/\"\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"ngan2710/voc-devkit-2007-and-2012\")\n",
        "\n",
        "scribble_aug_auto_zip_path = \"/content/dataset_augmentations/ScribbleAugAuto.zip\"\n",
        "segmentation_class_aug_zip_path = \"/content/dataset_augmentations/SegmentationClassAug.zip\"\n",
        "segmentation_object_aug_zip_path = \"/content/dataset_augmentations/SegmentationObjectAug.zip\"\n",
        "\n",
        "extract_to = \"/root/.cache/kagglehub/datasets/ngan2710/voc-devkit-2007-and-2012/versions/1/VOCdevkit/VOC2012/\"\n",
        "\n",
        "with zipfile.ZipFile(scribble_aug_auto_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "with zipfile.ZipFile(segmentation_class_aug_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "with zipfile.ZipFile(segmentation_object_aug_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "\n",
        "segmentation_replacement_zip_path = \"/content/dataset_augmentations/Segmentation.zip\"\n",
        "imagesets_dir = \"/root/.cache/kagglehub/datasets/ngan2710/voc-devkit-2007-and-2012/versions/1/VOCdevkit/VOC2012/ImageSets/\"\n",
        "\n",
        "shutil.rmtree(f\"{imagesets_dir}Segmentation\", ignore_errors=True)\n",
        "\n",
        "with zipfile.ZipFile(segmentation_replacement_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(imagesets_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "flUr2vr7FgDP",
        "outputId": "9df0ebf2-b0c0-4285-a3a9-c8a7735dfe8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TJZy_YkYwNMkdtECAlkWy8VuorPFjP1t\n",
            "From (redirected): https://drive.google.com/uc?id=1TJZy_YkYwNMkdtECAlkWy8VuorPFjP1t&confirm=t&uuid=46efc206-4ef4-46c7-a936-2eb3db5134b7\n",
            "To: /content/pretrained_model/vgg16-397923af.pth\n",
            "100% 553M/553M [00:09<00:00, 60.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ZP6FHiclSNk1nH0WxhCH-W_fwd3GnEZa\n",
            "From (redirected): https://drive.google.com/uc?id=1ZP6FHiclSNk1nH0WxhCH-W_fwd3GnEZa&confirm=t&uuid=fd29483a-8768-4c58-a63a-69150a3d0232\n",
            "To: /content/dataset_augmentations/ScribbleAugAuto.zip\n",
            "100% 40.5M/40.5M [00:01<00:00, 36.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oGy-tg_Tv_-ZUQeiNtM5i4sK0lYB0MeL\n",
            "To: /content/dataset_augmentations/Segmentation.zip\n",
            "100% 137k/137k [00:00<00:00, 4.10MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1R5G3BfQTAY4zWkazTsgFc3kJNQVjwi4M\n",
            "From (redirected): https://drive.google.com/uc?id=1R5G3BfQTAY4zWkazTsgFc3kJNQVjwi4M&confirm=t&uuid=96350615-7e79-48c6-a772-1a3931d6acc2\n",
            "To: /content/dataset_augmentations/SegmentationClassAug.zip\n",
            "100% 27.0M/27.0M [00:00<00:00, 89.2MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1D4oX0Ub6ObFO81yonWY4ehZ-tr5Hn5UK\n",
            "From (redirected): https://drive.google.com/uc?id=1D4oX0Ub6ObFO81yonWY4ehZ-tr5Hn5UK&confirm=t&uuid=926d3f82-7ec8-4aae-b9bc-6cf7e048d830\n",
            "To: /content/dataset_augmentations/SegmentationObjectAug.zip\n",
            "100% 28.6M/28.6M [00:00<00:00, 38.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing\n",
        "\n",
        "Recall that we split the dataset into 4 splits. Below, we use 1-way, 5-shot learning to train a model, utilizing PAR, using dense (strong) annotations.\n",
        "Afterwards, we evaluate the model on the remainder of the data to obtain binary-IoU and mean-IoU metrics.\n",
        "\n",
        "In the following sections, we will be using data from both training and testing to replicate the figures on the original paper.\n",
        "\n",
        "\n",
        "\n",
        "****\n",
        "**NOTE** that below, we only perform 1 'run'.\n",
        "From experience, a singular run (training and testing) takes roughly 1h 30min to 2hrs to run using an A100 GPU. In total, there are 72 of such 'runs'.\n",
        "\n",
        "As such, for simplicity and to be able to demonstrate that the pipeline works, only one 'run' will be used. If you wish to perform the **full** experiment, another `.ipynb` in the repository contains the code to do so.\n",
        "****"
      ],
      "metadata": {
        "id": "d8BjueMPJy5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/\")\n",
        "!python train.py with mode='train' dataset='VOC' label_sets=0 model.align=True task.n_ways=1 task.n_shots=5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3WTlAMkGJ1H",
        "outputId": "bde8b61d-72a9-4cc1-ac53-9ed2c72939cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - PANet - Running command 'main'\n",
            "INFO - PANet - Started run with ID \"1\"\n",
            "INFO - main - ###### Create model ######\n",
            "INFO - main - ###### Load data ######\n",
            "INFO - main - \n",
            "INFO - main - ###### Training Config ######\n",
            "INFO - main - n-ways:\t\t1\n",
            "INFO - main - n-shots:\t\t5\n",
            "INFO - main - n-queries:\t1\n",
            "INFO - main - PAR?:\t\tTrue\n",
            "INFO - main - \n",
            "INFO - main - ###### Set optimizer ######\n",
            "INFO - main - ###### Training ######\n",
            "0it [00:00, ?it/s]\n",
            "ERROR - PANet - Failed after 0:00:02!\n",
            "Traceback (most recent calls WITHOUT Sacred internals):\n",
            "  File \"/content/train.py\", line 116, in main\n",
            "    for i_iter, sample_batched in tqdm.tqdm(enumerate(trainloader)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "    return self._process_data(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "    raise exception\n",
            "FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/content/dataloaders/common.py\", line 165, in __getitem__\n",
            "    sample = [self.datasets[dataset_idx][data_idx]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/dataloaders/common.py\", line 165, in <listcomp>\n",
            "    sample = [self.datasets[dataset_idx][data_idx]\n",
            "              ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
            "  File \"/content/dataloaders/common.py\", line 197, in __getitem__\n",
            "    return self.dataset[self.indices[idx]]\n",
            "           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/dataloaders/pascal.py\", line 48, in __getitem__\n",
            "    image = Image.open(os.path.join(self._image_dir, f'{id_}.jpg'))\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3465, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/ngan2710/voc-devkit-2007-and-2012/versions/1/VOCdevkit/VOC2012/JPEGImages/2008_008567.jpg'\n",
            "\n",
            "\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ade1bfa27a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1582, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 940, in wait\n",
            "    with _WaitSelector() as selector:\n",
            "  File \"/usr/lib/python3.11/selectors.py\", line 202, in __exit__\n",
            "    def __exit__(self, *args):\n",
            "\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/\")\n",
        "!python test.py with mode='test' snapshot='./runs/PANet_VOC_align_sets_0_1way_5shot_[train]/1/snapshots/30000.pth'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqMlxDfdGMVu",
        "outputId": "59a6928e-fdd3-4729-d8db-aa89be539ee8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object address  : 0x7b2834117dc0\n",
            "object refcount : 2\n",
            "object type     : 0x9d5ea0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "if DRIVE_MOUNTED is False:\n",
        "  drive.mount('/content/drive')\n",
        "  DRIVE_MOUNTED = True\n",
        "\n",
        "final_model_weights = \"/content/runs/PANet_VOC_align_sets_0_1way_5shot_[train]/1/snapshots/30000.pth\"\n",
        "\n",
        "if DRIVE_MOUNTED is True:\n",
        "  src = \"/content/runs/\"\n",
        "  dst = \"/content/drive/MyDrive/PANet/Results/\"\n",
        "  shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "  final_model_weights = \"/content/drive/MyDrive/PANet/Results/PANet_VOC_align_sets_0_1way_5shot_[train]/1/snapshots/30000.pth\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-niNpECF2dB",
        "outputId": "fe47d8be-aa05-457d-83d1-8c0b732b3fcd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Evaluation Results\n",
        "We have only finished one training run.\n",
        "That in itself is sufficient to obtain a model that can perform semantic segmentation.\n",
        "\n",
        "\n",
        "However, in order to obtain the following:\n",
        "- Mean-IoU metrics for 1-shot and 5-shot learning\n",
        "- Binary-IoU metrics for 1-shot and 5-shot learning\n",
        "- Loss per iteration for 1-shot/5-shot learning w/ and w/out PAR\n",
        "- Mean-IoU metrics for models w/ and w/out PAR\n",
        "- Weak annotation metrics for 1-shot/5-shot models\n",
        "\n",
        ", which can be seen in the report, you would need to complete the remaining 71 runs. The snapshots generated by the training/testing contain loss and performance metrics. Analyzing results from these files are simple enough.\n",
        "\n",
        "For more details, check the aforementioned `full` notebook"
      ],
      "metadata": {
        "id": "5oWznrEiKRnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "\n",
        "def show_segmentation(idx, sample, query_pred) :\n",
        "  \"\"\" Plots a support and query set with the predicted mask.\n",
        "  \"\"\"\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,6), gridspec_kw={'wspace':0.3, 'hspace':0.2})\n",
        "\n",
        "  # display configs\n",
        "  do_threshold = True\n",
        "  threshold = 0.7\n",
        "  mask_alpha = 0.5\n",
        "  query_cmap = plt.get_cmap('bwr').reversed()\n",
        "  support_cmap = plt.get_cmap('bwr')\n",
        "\n",
        "  def process_mask(mask, do_threshold=False, threshold=0.7):\n",
        "    if do_threshold:\n",
        "      x = mask\n",
        "      norm_x = (x - x.min()) / (x.max() - x.min())\n",
        "      mask = (norm_x > threshold).float()\n",
        "      mask_np = mask.detach().cpu().numpy()\n",
        "      mask_np = np.ma.masked_where(mask_np == 1, mask_np)\n",
        "    else:\n",
        "      x = mask\n",
        "      mask = torch.sigmoid(x)\n",
        "      mask_np = mask.detach().cpu().numpy()\n",
        "      mask_np = np.ma.masked_where(mask_np == 1, mask_np)\n",
        "\n",
        "    return mask_np\n",
        "\n",
        "\n",
        "  # support image with ground truth segmentation mask\n",
        "  plt.subplot(1, 3, 1)\n",
        "  img = sample[\"support_images_t\"][0][0].squeeze(0)\n",
        "  img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "  mask = sample[\"support_mask\"][0][0][\"fg_mask\"].squeeze(0).cpu().numpy()\n",
        "  mask = np.ma.masked_where(mask == 0, mask)\n",
        "\n",
        "  plt.imshow(img_np)\n",
        "  plt.imshow(mask, cmap=support_cmap, alpha=mask_alpha)\n",
        "  plt.axis('off')\n",
        "\n",
        "\n",
        "  # ground truth segmentation mask\n",
        "  plt.subplot(1, 3, 2)\n",
        "  img = sample[\"query_images_t\"][0][0].squeeze(0)\n",
        "  img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "  mask = sample[\"query_masks\"][0][0].squeeze(0).squeeze(0).cpu().numpy()\n",
        "  mask = np.ma.masked_where(mask == 1, mask)\n",
        "\n",
        "  plt.imshow(img_np)\n",
        "  plt.imshow(mask, cmap=query_cmap, alpha=mask_alpha)\n",
        "  plt.axis('off')\n",
        "\n",
        "\n",
        "  # predicted segmentation mask\n",
        "  plt.subplot(1, 3, 3)\n",
        "  mask = process_mask(query_pred[0][0], do_threshold=do_threshold, threshold=threshold)\n",
        "  plt.imshow(img_np)\n",
        "  plt.imshow(mask, cmap=query_cmap, alpha=mask_alpha)\n",
        "  plt.axis('off')\n",
        "\n",
        "\n",
        "  # adds fancy stuff to make it look more like the report\n",
        "  pos = [ax.get_position() for ax in axes]\n",
        "  xmin = min(p.x0 for p in pos)\n",
        "  ymin = min(p.y0 for p in pos)\n",
        "  xmax = max(p.x1 for p in pos)\n",
        "  ymax = max(p.y1 for p in pos)\n",
        "\n",
        "  box = patches.FancyBboxPatch(\n",
        "      (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "      boxstyle=\"round,pad=0.011\",\n",
        "      transform=fig.transFigure,\n",
        "      fill=False,\n",
        "      edgecolor='black',\n",
        "      linewidth=1\n",
        "  )\n",
        "  fig.add_artist(box)\n",
        "\n",
        "  fig.text(0.515, 0.73, 'ground truth', ha='center', va='bottom')\n",
        "  fig.text(0.8, 0.73, 'prediction', ha='center', va='bottom')\n",
        "\n",
        "  fig.text(0.1, 0.73, f'{idx}', ha='center', va='bottom')\n",
        "\n",
        "  label = VOC_CLASS_STRS[int(sample[\"class_ids\"][0][0])]\n",
        "  fig.text(0.1, 0.5, label, rotation='vertical', ha='center', va='center')\n",
        "\n",
        "  return plt.gcf()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AJeZMMbSH6_-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Qualitative Examples\n",
        "# @markdown This block will perform segmentation on 'IMAGES_TO_DISPLAY' images.\n",
        "\n",
        "IMAGES_TO_DISPLAY = 5 # @param {type:\"integer\"}\n",
        "VOC_CLASS_STRS = [\n",
        "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\",\n",
        "    \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n",
        "\n",
        "# manually init some of the config variables\n",
        "# sometimes they don't get loaded properly\n",
        "_config = {}\n",
        "_config['optim'] = {\n",
        "  'lr': 1e-3,\n",
        "  'momentum': 0.9,\n",
        "  'weight_decay': 0.0005,\n",
        "}\n",
        "_config['model'] = {\n",
        "  'align': True,\n",
        "}\n",
        "_config['task'] = {\n",
        "  'n_ways': 1,\n",
        "  'n_shots': 1,\n",
        "  'n_queries': 1,\n",
        "}\n",
        "_config['n_steps'] = 30000\n",
        "_config['label_sets'] = 0\n",
        "_config['batch_size'] = 1\n",
        "_config['lr_milestones'] = [10000, 20000, 30000]\n",
        "_config['align_loss_scaler'] = 1\n",
        "_config['ignore_label'] = 255\n",
        "_config['print_interval'] = 100\n",
        "_config['save_pred_every'] = 1_000  # was 10_000\n",
        "_config[\"input_size\"] = (417, 417)\n",
        "_config[\"seed\"] = 1234\n",
        "_config[\"cuda_visable\"] = '0, 1, 2, 3, 4, 5, 6, 7'\n",
        "_config[\"gpu_id\"] = 0\n",
        "_config[\"mode\"] = 'test' # 'train' or 'test'\n",
        "\n",
        "\n",
        "# init torch, model, seeds, etc.\n",
        "set_seed(_config['seed'])\n",
        "cudnn.enabled = True\n",
        "cudnn.benchmark = True\n",
        "torch.cuda.set_device(device=_config['gpu_id'])\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "model = FewShotSeg(pretrained_path=final_model_weights, cfg=_config['model'])\n",
        "model = nn.DataParallel(model.cuda(), device_ids=[_config['gpu_id'],])\n",
        "model.load_state_dict(torch.load(final_model_weights, map_location='cpu'))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# init dataset\n",
        "data_name = \"VOC\"\n",
        "if data_name == 'VOC':\n",
        "    make_data = voc_fewshot\n",
        "    max_label = 20\n",
        "elif data_name == 'COCO':\n",
        "    make_data = coco_fewshot\n",
        "    max_label = 80\n",
        "else:\n",
        "    raise ValueError('Wrong config for dataset!')\n",
        "labels = CLASS_LABELS[data_name]['all'] - CLASS_LABELS[data_name][_config['label_sets']]\n",
        "transforms = [Resize(size=_config['input_size'])]\n",
        "transforms = Compose(transforms)\n",
        "\n",
        "dataset = make_data(\n",
        "    base_dir=\"/root/.cache/kagglehub/datasets/ngan2710/voc-devkit-2007-and-2012/versions/1/VOCdevkit/VOC2012/\",\n",
        "    split=\"trainaug\",\n",
        "    transforms=transforms,\n",
        "    to_tensor=ToTensorNormalize(),\n",
        "    labels=labels,\n",
        "    max_iters=1_000 * 1,\n",
        "    n_ways=1,\n",
        "    n_shots=1,\n",
        "    n_queries=1\n",
        ")\n",
        "\n",
        "testloader = DataLoader(dataset, batch_size=_config['batch_size'], shuffle=False, num_workers=1, pin_memory=True, drop_last=False)\n",
        "\n",
        "\n",
        "for idx, sample in enumerate(testloader):\n",
        "  # passing the input into the model\n",
        "  # note that we cant just do 'model(sample)' because we have to account for\n",
        "  # different types of annotaions.\n",
        "  _config['bbox'] = False\n",
        "  _config['scribble'] = False\n",
        "  label_ids = list(sample['class_ids'])\n",
        "  support_images = [[shot.cuda() for shot in way] for way in sample['support_images']]\n",
        "  suffix = 'scribble' if _config['scribble'] else 'mask'\n",
        "\n",
        "  if _config['bbox']:\n",
        "      support_fg_mask = []\n",
        "      support_bg_mask = []\n",
        "      for i, way in enumerate(sample['support_mask']):\n",
        "          fg_masks = []\n",
        "          bg_masks = []\n",
        "          for j, shot in enumerate(way):\n",
        "              fg_mask, bg_mask = get_bbox(shot['fg_mask'], sample['support_inst'][i][j])\n",
        "              fg_masks.append(fg_mask.float().cuda())\n",
        "              bg_masks.append(bg_mask.float().cuda())\n",
        "          support_fg_mask.append(fg_masks)\n",
        "          support_bg_mask.append(bg_masks)\n",
        "  else:\n",
        "      support_fg_mask = [[shot[f'fg_{suffix}'].float().cuda() for shot in way] for way in sample['support_mask']]\n",
        "      support_bg_mask = [[shot[f'bg_{suffix}'].float().cuda() for shot in way] for way in sample['support_mask']]\n",
        "\n",
        "  query_images = [query_image.cuda() for query_image in sample['query_images']]\n",
        "  query_labels = torch.cat([query_label.cuda()for query_label in sample['query_labels']], dim=0)\n",
        "  query_pred, _ = model(support_images, support_fg_mask, support_bg_mask, query_images)\n",
        "\n",
        "  label = VOC_CLASS_STRS[int(sample[\"class_ids\"][0][0])]\n",
        "  figure = show_segmentation(idx, sample, query_pred)\n",
        "  plt.show(figure)\n",
        "\n",
        "  if idx >= IMAGES_TO_DISPLAY:\n",
        "    break"
      ],
      "metadata": {
        "id": "YVOLLtApxknE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}